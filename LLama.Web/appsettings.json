{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "LLamaOptions": {
    "ModelLoadType": 0,
    "Models": [
      {
        "Name": "Phi-3 Mini (Local)",
        "MaxInstances": 20,
        "ModelPath": "C:\\models\\phi-3-mini-4k-instruct-q4.gguf",
        "ContextSize": 4096,
        "BatchSize": 2048,
        "Threads": 4,
        "GpuLayerCount": 0,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": true,
        "Perplexity": false,
        "LoraAdapter": "",
        "LoraBase": "",
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      },
      {
        "Name": "Qwen2.5-1.5B (Local)",
        "ModelPath": "C:\\models\\Qwen2.5-1.5B-Instruct-Q6_K.gguf",
        "ContextSize": 8192,
        "GpuLayerCount": 0
      },
      {
        "Name": "TinyLlama-1.1b (Local)",
        "ModelPath": "C:\\models\\tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
        "ContextSize": 2048,
        "GpuLayerCount": 0
      }
    ]
  }
}
